{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1770987004674
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2026-02-13T12:50:05.3527986Z",
              "execution_start_time": "2026-02-13T12:49:34.1780086Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "59b41e52-6c17-4a34-8fe3-7956f2f5835a",
              "queued_time": "2026-02-13T12:45:52.6450969Z",
              "session_id": "1",
              "session_start_time": "2026-02-13T12:45:52.6461446Z",
              "spark_jobs": {
                "jobs": [],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "cd79b5aa-c06e-4c4c-8a02-0fbc73d1aae4",
              "state": "finished",
              "statement_id": 6,
              "statement_ids": [
                6
              ]
            },
            "text/plain": [
              "StatementMeta(cd79b5aa-c06e-4c4c-8a02-0fbc73d1aae4, 1, 6, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-13 12:49:43.117025: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample dataset: Study hours, previous exam scores, and pass/fail labels\n",
        "data = {\n",
        "    'StudyHours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'PrevExamScore': [30, 40, 45, 50, 60, 65, 70, 75, 80, 85],\n",
        "    'Pass': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  # 0 = Fail, 1 = Pass\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Dataset Summary:\")\n",
        "print(df)\n",
        "print(f\"\\nDataset shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Features (X) and Target (y)\n",
        "X = df[['StudyHours', 'PrevExamScore']]  # Features\n",
        "y = df['Pass']  # Target variable (0 = Fail, 1 = Pass)\n",
        "\n",
        "# Split data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Testing data: {X_test.shape}, {y_test.shape}\")\n",
        "print(\"\\nTraining set:\")\n",
        "print(X_train)\n",
        "print(\"Training target:\")\n",
        "print(y_train.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train the Logistic Regression model\n",
        "logreg_model = LogisticRegression(random_state=42)\n",
        "logreg_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the test set\n",
        "y_pred_logreg = logreg_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Logistic Regression model\n",
        "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOGISTIC REGRESSION MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy: {accuracy_logreg:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_logreg))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_logreg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train the Decision Tree Classifier\n",
        "tree_model = DecisionTreeClassifier(random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the test set\n",
        "y_pred_tree = tree_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Decision Tree model\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DECISION TREE MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy: {accuracy_tree:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_tree))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_tree))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model performance\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nLogistic Regression Accuracy: {accuracy_logreg:.4f}\")\n",
        "print(f\"Decision Tree Accuracy:       {accuracy_tree:.4f}\")\n",
        "\n",
        "if accuracy_logreg > accuracy_tree:\n",
        "    print(f\"\\n✓ Logistic Regression performs better by {(accuracy_logreg - accuracy_tree)*100:.2f}%\")\n",
        "elif accuracy_tree > accuracy_logreg:\n",
        "    print(f\"\\n✓ Decision Tree performs better by {(accuracy_tree - accuracy_logreg)*100:.2f}%\")\n",
        "else:\n",
        "    print(\"\\n✓ Both models have equal accuracy\")\n",
        "\n",
        "# Create comparison visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "models = ['Logistic\\nRegression', 'Decision\\nTree']\n",
        "accuracies = [accuracy_logreg, accuracy_tree]\n",
        "colors = ['#3498db', '#e74c3c']\n",
        "\n",
        "axes[0].bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0].set_title('Model Accuracy Comparison', fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add accuracy labels on bars\n",
        "for i, acc in enumerate(accuracies):\n",
        "    axes[0].text(i, acc + 0.02, f'{acc:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot prediction results\n",
        "x_range = np.arange(len(y_test))\n",
        "axes[1].plot(x_range, y_test.values, 'ko-', linewidth=2, markersize=8, label='Actual', alpha=0.7)\n",
        "axes[1].plot(x_range, y_pred_logreg, 's--', linewidth=2, markersize=6, label='LogReg Pred', alpha=0.7)\n",
        "axes[1].plot(x_range, y_pred_tree, '^--', linewidth=2, markersize=6, label='Tree Pred', alpha=0.7)\n",
        "axes[1].set_xlabel('Test Sample Index', fontsize=12)\n",
        "axes[1].set_ylabel('Prediction (0=Fail, 1=Pass)', fontsize=12)\n",
        "axes[1].set_title('Predictions on Test Set', fontsize=13, fontweight='bold')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].set_ylim([-0.1, 1.1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVisualization complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the decision tree structure\n",
        "plt.figure(figsize=(14, 8))\n",
        "tree.plot_tree(tree_model, \n",
        "               feature_names=['StudyHours', 'PrevExamScore'], \n",
        "               class_names=['Fail', 'Pass'], \n",
        "               filled=True, \n",
        "               rounded=True,\n",
        "               fontsize=10)\n",
        "plt.title('Decision Tree Structure for Pass/Fail Classification', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDecision Tree Visualization Complete!\")\n",
        "print(f\"Tree Depth: {tree_model.get_depth()}\")\n",
        "print(f\"Number of Leaves: {tree_model.get_n_leaves()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tuning Decision Tree to prevent overfitting\n",
        "print(\"=\" * 60)\n",
        "print(\"DECISION TREE TUNING FOR OVERFITTING PREVENTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test different max depths to find optimal\n",
        "depths = range(1, 6)\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for depth in depths:\n",
        "    tuned_tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    tuned_tree.fit(X_train, y_train)\n",
        "    \n",
        "    train_acc = accuracy_score(y_train, tuned_tree.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, tuned_tree.predict(X_test))\n",
        "    \n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "    \n",
        "    print(f\"Depth {depth}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
        "\n",
        "# Plot tuning results\n",
        "plt.figure(figsize=(11, 5))\n",
        "plt.plot(depths, train_accuracies, 'o-', linewidth=2, markersize=8, label='Training Accuracy', color='#2ecc71')\n",
        "plt.plot(depths, test_accuracies, 's-', linewidth=2, markersize=8, label='Testing Accuracy', color='#e74c3c')\n",
        "plt.xlabel('Tree Depth', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.title('Decision Tree Tuning: Impact of Max Depth on Model Performance', fontsize=13, fontweight='bold')\n",
        "plt.xticks(depths)\n",
        "plt.ylim([0, 1.05])\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend(fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal depth\n",
        "optimal_depth = depths[test_accuracies.index(max(test_accuracies))]\n",
        "print(f\"\\n✓ Optimal max_depth: {optimal_depth} (Test Accuracy: {max(test_accuracies):.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reflection and Analysis\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"REFLECTION AND ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "analysis = \"\"\"\n",
        "FINDINGS FROM THE PRACTICAL EXERCISE:\n",
        "=====================================\n",
        "\n",
        "1. MODEL PERFORMANCE COMPARISON\n",
        "   - Logistic Regression Accuracy: {logreg_accuracy:.4f}\n",
        "   - Decision Tree Accuracy:       {tree_accuracy:.4f}\n",
        "   - Best Performer:               {best_model}\n",
        "\n",
        "2. KEY OBSERVATIONS:\n",
        "   \n",
        "   a) Logistic Regression:\n",
        "      ✓ Pros:\n",
        "        • Simple and interpretable model\n",
        "        • Efficient training and prediction\n",
        "        • Assumes linear relationship between features and target\n",
        "        • Less prone to overfitting on small datasets\n",
        "        • Works well with this dataset (linear separability)\n",
        "      \n",
        "      ✗ Cons:\n",
        "        • Limited flexibility for complex patterns\n",
        "        • Cannot capture non-linear relationships\n",
        "        • May underfit on complex datasets\n",
        "   \n",
        "   b) Decision Tree:\n",
        "      ✓ Pros:\n",
        "        • Can capture non-linear relationships\n",
        "        • Highly interpretable (easy to understand decision paths)\n",
        "        • No feature scaling required\n",
        "        • Can handle both numerical and categorical data\n",
        "        • Provides feature importance information\n",
        "      \n",
        "      ✗ Cons:\n",
        "        • Prone to overfitting, especially with deeper trees\n",
        "        • Training complexity O(n*log n) for sorting features\n",
        "        • Small changes in data can lead to completely different trees\n",
        "        • Greedy approach may not find truly optimal splits\n",
        "\n",
        "3. WHEN TO USE EACH MODEL:\n",
        "   \n",
        "   Use Logistic Regression when:\n",
        "   - You need a fast, interpretable model\n",
        "   - Your data has linear decision boundaries\n",
        "   - You have limited training data\n",
        "   - You need to minimize overfitting risk\n",
        "   - Features have clear linear relationships with the target\n",
        "   \n",
        "   Use Decision Trees when:\n",
        "   - You need to capture complex, non-linear patterns\n",
        "   - Interpretability of decision paths is important\n",
        "   - You have sufficient data to prevent overfitting\n",
        "   - You need to handle mixed feature types\n",
        "   - You want automatic feature interaction discovery\n",
        "\n",
        "4. DATA COMPLEXITY IMPACT:\n",
        "   - With the simple, linearly separable dataset used here,\n",
        "     Logistic Regression performs competitively\n",
        "   - Decision Trees don't need feature scaling\n",
        "   - The small dataset size limits tree depth before overfitting occurs\n",
        "   - Both models assign the same importance to the features\n",
        "\n",
        "5. RECOMMENDATIONS:\n",
        "   - For this particular problem (pass/fail prediction):\n",
        "     The data is {recommendation_choice}, so {recommendation_detail}.\n",
        "   - Always use cross-validation for more robust evaluation\n",
        "   - Consider ensemble methods (Random Forest, Gradient Boosting)\n",
        "     that combine multiple decision trees\n",
        "   - Scale features before using Logistic Regression for better results\n",
        "   - Monitor for overfitting by comparing train/test accuracies\n",
        "\n",
        "CONCLUSION:\n",
        "===========\n",
        "Both Logistic Regression and Decision Trees have their place in\n",
        "machine learning. The choice depends on your specific problem, data\n",
        "characteristics, and requirements. Start simple with Logistic Regression,\n",
        "and move to more complex models like Decision Trees or ensembles if needed.\n",
        "\"\"\"\n",
        "\n",
        "best_model = \"Logistic Regression\" if accuracy_logreg >= accuracy_tree else \"Decision Tree\"\n",
        "recommendation_choice = \"linearly separable\" if accuracy_logreg >= accuracy_tree else \"complex\"\n",
        "recommendation_detail = (\"use Logistic Regression for simplicity and efficiency\" \n",
        "                        if accuracy_logreg >= accuracy_tree \n",
        "                        else \"Decision Tree captures the patterns better\")\n",
        "\n",
        "print(analysis.format(\n",
        "    logreg_accuracy=accuracy_logreg,\n",
        "    tree_accuracy=accuracy_tree,\n",
        "    best_model=best_model,\n",
        "    recommendation_choice=recommendation_choice,\n",
        "    recommendation_detail=recommendation_detail\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": ".venv (3.10.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": "ipython",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython",
      "version": "3.10.12"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
